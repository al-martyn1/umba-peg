---
Title : Архитектура
---

# PEG-парсер

Я пробовал несколько разных инструментов для генерации парсеров, но ни один из них меня не удовлетворил,
а изучать глубоко какие-то новые, типа ANTLR - как-то уже не хочется, поверхностный взгляд подсказывает, 
что там тоже не так всё красиво.

PEG выглядит довольно интересной идеей, чтобы попробовать сделать это самому.


## Рекурсивный спуск

Классический рекурсивный спуск предлагает нам писать руками функции разбора альтернатив, и выбирает по 
токену на входе для продолжения разбора вызов соответствующей альтернативе функции. Если разбор альтернативы 
не удаётся, то производится попытка разобрать следующую альтернативу. Первые языки я именно так и писал, но 
это плохо автоматизируется и требует большого количества ручной работы, а также занимает стек, который
даже сейчас довольно ограничен.

Классический рекурсивный спуск извлекает токены по одному, а история просмотра сохраняется в стековых 
фреймах функций по всей цепочке вложенных вызовов.

Но можно поступить по-другому - сохранить все токены в списке (массиве), и при необходимости поремещаться 
по нему как вперёд, так и назад, а в точках ветвления запоминать уже проверенные альтернативы и при откате 
перемещаться по новым путям. Именно такой подход и предлагает PEG.


## Используем токенизатор

Целиком делать PEG-парсер задача не из самых простых, и у нас уже есть токенизатор. 
Он не само совершенство, но более или менее работает.

Токенизатор был первым кирпичиком, когда я не знал, в какую сторону мне смотреть для создания парсеров языков.

Токенизатор может быть довольно просто настроен (или в более сложных случаях - еще и дописан) для токенизации 
любого языка. Так, существуют хелперы для токенизации C++ (там, правда, пока отсутствует поддержка новых типов 
литералов C++).

Токенизатор уже сейчас позволяет делать подсветку синтаксиса.

Также, изучая статьи Guido van Rossum по PEG-парсеру для питона (в русском переводе на хабре), я увидел, 
что он пошел по такому же пути. И точно также он не знал как быть, и пошел, куда глядят глаза. Так и я пойду, 
куда глядят глаза, а по дороге - разберусь что к чему (и возможно, даже переделаю первую получившиюся реализацию, 
но это не точно; но, как минимум, я точно захочу её переделать).


## Отложенная токенизация.

Guido van Rossum подкидывает нам идею:

> Я полагаю, вы могли бы просто сначала токенизировать весь ввод в список, а затем использовать его в качестве 
входных данных синтаксического анализатора. Но если бы в конце файла был недействительный токен (например, строка 
с отсутствующей закрывающей кавычкой) и ранее в файле также была синтаксическая ошибка, то сначала вы получите 
сообщение об ошибке от токенизатора. Я считаю, что это плохо для пользователя, так как синтаксическая ошибка может 
быть основной причиной некорректной строки. Так что у меня несколько иные требования к токенизатору, в частности, 
он должен быть реализован как ленивый список.

Базовый API очень прост. Объект Tokenizer инкапсулирует массив токенов и позицию в этом массиве. У него есть три 
основных метода:

- getToken() возвращает следующий токен, сдвигая указатель (или считывет следующий токен из источника, если мы 
  находимся в конце буфера токенов);
- getTokenPos() возвращает текущую позицию в буфере токенов;
- reset(pos) устанавливает позицию в буфере (аргумент должен быть получен из mark());
- peekToken() возвращает следующий токен без сдвига позиции в буфере.


Как это делаем. У нас есть:

- текст файла с исходниками разбираемого языка;
- пара итераторов: it - текущий итератор, и itEnd - указывает за конец;
- массив принятых токенов;
- настроенный TokenizerHandler, который складывает принимаемые токены в массив.

Как это работает. 

- `getToken()`/`peekToken()` при наличии в массиве доступного токена возвращают его.
  Если это токен конца, обрабатываем ситуацию "конец потока".

- Если доступного токена нет:

  - пока `it!=itEnd` скармливаем TokenizerHandler'у значение *it. Как только TokenizerHandler
    поймал токен, помещаем его в массив токенов, завершаем цикл и возвращаем вновь пришедший токен.

  - если достигли условия окончания входного потока `it==itEnd`, скармливаем ткенизеру события `finalize`.
    Тут либо токенизер выплюнет ошибку, либо TokenizerHandler выплюнет токен окончания потока.
    Если не произошло ни того, ни другого, то у нас токенизация накривячила где-то, и надо выкинуть 
    исключение/assert. Иначе обрабатываем либо ошибку токенизатора, либо обрабатываем ситуацию "конец потока".

На пальцах - примерно так.




