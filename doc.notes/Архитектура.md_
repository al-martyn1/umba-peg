---
Title : Архитектура
---

# PEG-парсер

Я пробовал несколько разных инструментов для генерации парсеров, но ни один из них меня не удовлетворил, а изучать глубоко какие-то новые, типа ANTLR
- как-то уже не хочется, поверхностный же взгляд подсказывает, что там тоже не так всё красиво.

PEG выглядит довольно интересной идеей, чтобы попробовать сделать это самому.


## Рекурсивный спуск

Классический рекурсивный спуск предлагает нам писать руками функции разбора альтернатив, и выбирает по токену на входе для продолжения разбора 
вызов соответствующей альтернативе функции. Если разбор альтернативы не удаётся, то производится попытка разобрать следующую альтернативу.
Первые языки я именно так и писал, но это плохо автоматизируется и требует большого количества ручной работы, а также занимает стек, который
даже сейчас довольно ограничен.

Классический рекурсивный спуск извлекает токены по одному, а история просмотра сохраняется в стековых фреймах функций по всеё цепочке вложенных вызовов.

Но можно поступить по-другому - сохранить все токены в списке (массиве), и при необходимости поремещаться по нему как вперёд, так и назад, а в точках ветвления
запоминать уже проверенные альтернативы и при откате перемещаться по новым путям. Именно такой подход и предлагает PEG.


## Используем токенизатор

Целиком делать PEG-парсер задача не из самых простых, и у нас уже есть токенизатор. 
Он не само совершенство, но более или менее работает.

Токенизатор был первым кирпичиком, когда я не знал, в какую сторону мне смотреть для создания парсеров языков.

Токенизатор может быть довольно просто настроен (или в более сложных случаях - еще и дописан) для токенизации любого языка.
Так, существуют хелперы для токенизации C++ (там, правда, пока отсутствует поддержка новых типов литералов C++).

Токенизатор уже сейчас позволяет делать подсветку синтаксиса.

Также, изучая статьи Guido van Rossum по PEG-парсеру для питона (в русском переводе на хабре), я увидел, 
что он пошел по такому же пути. И точно также он не знал как быть, и пошел, куда глядят глаза. Так и я пойду, куда глядят
глаза, а по дороге - разберусь что к чему (и возможно, даже переделаю первую получившиюся реализацию, но это не точно; но, как минимум,
я точно захочу её переделать).


## Отложенная токенизация.

Guido van Rossum подкидывает нам идею:

> Я полагаю, вы могли бы просто сначала токенизировать весь ввод в список, а затем использовать его в качестве входных данных 
синтаксического анализатора. Но если бы в конце файла был недействительный токен (например, строка с отсутствующей закрывающей 
кавычкой) и ранее в файле также была синтаксическая ошибка, то сначала вы получите сообщение об ошибке от токенизатора. Я считаю, 
что это плохо для пользователя, так как синтаксическая ошибка может быть основной причиной некорректной строки. Так что у меня несколько 
иные требования к токенизатору, в частности, он должен быть реализован как ленивый список.


